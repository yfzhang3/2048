{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yfzhang3/2048/blob/main/v2_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35826946",
      "metadata": {
        "id": "35826946"
      },
      "source": [
        "### Load & Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "#\n",
        "import sys\n",
        "sys.path.append('./sample_data/')\n",
        "\n",
        "# from challenge_dataset import ChallengeDataset\n",
        "# I don't know where this notebook is so idk which directory this belongs in"
      ],
      "metadata": {
        "id": "e3oFrDx_cvEU"
      },
      "id": "e3oFrDx_cvEU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e86449",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "76e86449",
        "outputId": "58350ab8-c881-4500-b90a-3107040e18a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c9a8f47c9ecd>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load training file data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtraining_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.txt'"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')  # Download the tokenizer data (only need to do this once)\n",
        "\n",
        "torch.manual_seed(1)  # set a manual seed for training reproducibility for random parameters\n",
        "\n",
        "# Load training file data\n",
        "training_path = 'train.txt'\n",
        "with open(training_path, 'r', encoding='utf-8', errors='replace') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Split data into original and corrupted at tab\n",
        "data = [line.strip().split('\\t') for line in lines]\n",
        "print(data[:5])\n",
        "\n",
        "# Separate original and corrupted\n",
        "original_sentences = [pair[0] for pair in data]\n",
        "corrupted_sentences = [pair[1] for pair in data]\n",
        "print(original_sentences[:3])\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = []\n",
        "for sentence in original_sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokenized_sentences.append(tokens)\n",
        "\n",
        "num_sentences = len(tokenized_sentences)\n",
        "print(num_sentences)\n",
        "print(tokenized_sentences[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenize Data"
      ],
      "metadata": {
        "id": "Mqf62idarTcI"
      },
      "id": "Mqf62idarTcI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Customize hyperparameters\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMDEDDING_DIM = 100\n",
        "\n",
        "# Flatten data into 1 list\n",
        "all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
        "print(all_words[:2])\n",
        "\n",
        "# Deriving a set from `train_set` to remove duplicates\n",
        "vocab = set(all_words)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# dictionary that maps words to their corresponding indices\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "# dictionary that maps indices to their corresponding words\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "# Creating training data\n",
        "data = []\n",
        "for sentence in tokenized_sentences:\n",
        "  for i in range(CONTEXT_SIZE, len(sentence) - CONTEXT_SIZE):\n",
        "      context = (\n",
        "          sentence[i - CONTEXT_SIZE:i]  # Context words to the left\n",
        "          + sentence[i + 1:i + CONTEXT_SIZE + 1]  # Context words to the right\n",
        "      )\n",
        "      target = sentence[i]\n",
        "      data.append((context, target))\n",
        "\n",
        "print(data[:5])"
      ],
      "metadata": {
        "id": "xwyWAE42rSrn"
      },
      "id": "xwyWAE42rSrn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training & validation but currently not working\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random_seed = 3\n",
        "random.seed(random_seed)\n",
        "random.shuffle(data)\n",
        "\n",
        "total_sentences_len = len(data)\n",
        "print(\"length of tokenized sentenences\", total_sentences_len)\n",
        "\n",
        "# Define the proportions for training, validation, and testing sets\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "\n",
        "# Calculate the sizes of each subset based on the ratios\n",
        "train_size = int(train_ratio * num_sentences)\n",
        "val_size = int(val_ratio * num_sentences)\n",
        "# Split the tokenized sentences into three subsets\n",
        "train_set = data[:train_size]\n",
        "val_set = data[train_size:train_size + val_size]\n",
        "test_set = data[train_size + val_size:]"
      ],
      "metadata": {
        "id": "dwDWtu6NUtl7"
      },
      "id": "dwDWtu6NUtl7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Neural Network"
      ],
      "metadata": {
        "id": "z6CSxdFXrgkM"
      },
      "id": "z6CSxdFXrgkM"
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "\n",
        "    # Params for vocab size, embedding dem, batch size\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        #out: 1 x emdedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "\n",
        "        #out: 1 x vocab_size\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        context_vector = torch.sum(embeds, dim=0).view(1, -1) # Summing word embeddings\n",
        "        print(context_vector.shape) # torch.Size([1, 100])\n",
        "        out = self.linear1(embeds) # now a number per node 1x128\n",
        "        out = self.activation_function1(out) # relu ->number 1x128\n",
        "        out = self.linear2(out) # 1 x voc sz\n",
        "        out = self.activation_function2(out) #soft max, probs sum = 1\n",
        "        return out\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1,-1)\n",
        "\n",
        "# Functions from pytorch docs\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)  # example"
      ],
      "metadata": {
        "id": "CpTiXnbGrfk5"
      },
      "id": "CpTiXnbGrfk5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating, Training, and Evaluating the Model"
      ],
      "metadata": {
        "id": "ZbkRD0g7gfYq"
      },
      "id": "ZbkRD0g7gfYq"
    },
    {
      "cell_type": "code",
      "source": [
        "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "\n",
        "#TRAINING\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "\n",
        "    for context, target in data:\n",
        "        # Vector mapping each word in context to appropriate index\n",
        "        context_vector = make_context_vector(context, word_to_ix)\n",
        "        # 1D Tensor (1,) holding the index of the target word * len of context vector\n",
        "        target_tensor = torch.tensor([word_to_ix[target]] * len(context_vector))\n",
        "\n",
        "        # Passes context vector through model -> returns predicted log probabilities\n",
        "        log_probs = model(context_vector)\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        # Cumulative loss over all instances in the current epoch\n",
        "        # used in backprop\n",
        "        total_loss += loss\n",
        "\n",
        "    #optimize at the end of each epoch\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"finish training\")\n",
        "\n",
        "    # VALIDATION (after each epoch)\n",
        "\n",
        "    model.eval() # Set model to evaulation mode\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions = []  # Store model predictions for precision/recall\n",
        "\n",
        "        for context, target in val_set:\n",
        "            # Generates vector, predicats lob probs\n",
        "            context_vector = make_context_vector(context, word_to_ix)\n",
        "            target_idx = word_to_ix[target]\n",
        "            log_probs = model(context_vector)\n",
        "            probabilities = torch.exp(log_probs) # convert log to regular\n",
        "\n",
        "            # Gets the predicted index w highest log prob\n",
        "            _, predicted = log_probs.max(1)\n",
        "\n",
        "            # Checks if prediction is correct\n",
        "            if predicted.item() == target_idx:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "        accuracy = correct / total  # Calculate accuracy for the validation set\n",
        "        print(f\"Accuracy (Validation) - Epoch {epoch + 1}: {accuracy:.4f}\")\n",
        "        print(\"Finish training\")\n",
        "\n",
        "    model.train()  # Set the model back to training mode"
      ],
      "metadata": {
        "id": "RmH9wehMenms"
      },
      "id": "RmH9wehMenms",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}